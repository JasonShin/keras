{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot as plot\n",
    "import keras.preprocessing.text\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras.preprocessing.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "max_review_length = 500\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding number of classes\n",
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 223s 9ms/step - loss: 0.4933 - acc: 0.7645 - val_loss: 0.3598 - val_acc: 0.8483\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 221s 9ms/step - loss: 0.3133 - acc: 0.8735 - val_loss: 0.3309 - val_acc: 0.8627\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 217s 9ms/step - loss: 0.2630 - acc: 0.8974 - val_loss: 0.3048 - val_acc: 0.8759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181f50f3d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "# Uses 32 vectors to represent each word\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# LSTM with 100 memory unit\n",
    "model.add(LSTM(100))\n",
    "# It's a binary classification issue. Use single neuron to output either 0 or 1\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...,   14    6  717]\n",
      " [   0    0    0 ...,  125    4 3077]\n",
      " [  33    6   58 ...,    9   57  975]\n",
      " ..., \n",
      " [   0    0    0 ...,   21  846    2]\n",
      " [   0    0    0 ..., 2302    7  470]\n",
      " [   0    0    0 ...,   34 2005 2643]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 56s 2ms/step\n",
      "Accuracy: 87.588\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: {}'.format(scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "I've enjoyed previous Thor movies and after seeing the rating here i expected this to be a decent movie, it wasn't.\n",
    "\n",
    "I guess this is the trend to make money on movies now days, just have big stars, bad jokes and lot of pointless action and effects. It's just so sad if you think about the potential of how good these movies could be.\n",
    "\n",
    "Maybe this was the last Marvel movie I bother to watch.\n",
    "'''\n",
    "x = keras.preprocessing.text.one_hot(text, top_words, lower=True, split=' ')\n",
    "x = [x]\n",
    "x = sequence.pad_sequences(x, max_review_length)\n",
    "predictions = model.predict_classes(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "('Someone likes the movie: ', \"\\nI've enjoyed previous Thor movies and after seeing the rating here i expected this to be a decent movie, it wasn't.\\n\\nI guess this is the trend to make money on movies now days, just have big stars, bad jokes and lot of pointless action and effects. It's just so sad if you think about the potential of how good these movies could be.\\n\\nMaybe this was the last Marvel movie I bother to watch.\\n\")\n"
     ]
    }
   ],
   "source": [
    "sentiment = predictions[0][0]\n",
    "print(predictions)\n",
    "if sentiment == 1:\n",
    "    print('Someone likes the movie: ', text)\n",
    "else:\n",
    "    print('Someone DOESNT like the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
